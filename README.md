# VAE
This report presents an extensive investigation into Variational Autoencoders (VAEs), a powerful class of generative models. 
I made an in-depth exploration of the mathematical formulations underpinning VAE and achieved a comprehensive understanding 
of latent models. I implemented VAE and tested it on MNIST and FashionMNIST dataset. A comparative analysis was performed 
between MLP-based and CNN-based encoder-decoder architectures, as well as different latent space dimensions, revealing insights 
into how specific network architectures can influence overall performance.
